[
  {
    "objectID": "Assignment_Report.html",
    "href": "Assignment_Report.html",
    "title": "Comprehensive Machine Learning Reoprt",
    "section": "",
    "text": "This research report consolidates multiple machine learning assignments performed across various datasets. The key goal was to explore supervised and unsupervised learning techniques to derive insights, build predictive models, and understand data-driven decision-making processes."
  },
  {
    "objectID": "Assignment_Report.html#executive-summary",
    "href": "Assignment_Report.html#executive-summary",
    "title": "Comprehensive Machine Learning Reoprt",
    "section": "",
    "text": "This research report consolidates multiple machine learning assignments performed across various datasets. The key goal was to explore supervised and unsupervised learning techniques to derive insights, build predictive models, and understand data-driven decision-making processes."
  },
  {
    "objectID": "Assignment_Report.html#key-insights",
    "href": "Assignment_Report.html#key-insights",
    "title": "Comprehensive Machine Learning Reoprt",
    "section": "Key Insights",
    "text": "Key Insights\n\nRegression analysis on insurance data achieved low explanatory power, indicating need for feature enhancement.\nClustering on Iris dataset effectively separated species using K-Means and Agglomerative algorithms.\nAssociation Rule Mining confirmed petal dimensions as strong predictors of Iris species.\nData preprocessing significantly improved dataset quality for modeling."
  },
  {
    "objectID": "Assignment_Report.html#conclusions",
    "href": "Assignment_Report.html#conclusions",
    "title": "Comprehensive Machine Learning Reoprt",
    "section": "Conclusions",
    "text": "Conclusions\n\nMachine learning algorithms provide valuable insight when data is well-prepared and feature-rich.\nRegression performance highlights importance of feature engineering.\nClustering and association results confirm algorithmic efficiency on structured data.\nFuture studies should improve model tuning and dataset variety for better generalization."
  },
  {
    "objectID": "Assignment_Report.html#introduction",
    "href": "Assignment_Report.html#introduction",
    "title": "Comprehensive Machine Learning Reoprt",
    "section": "Introduction",
    "text": "Introduction\nThis report covers key ML domains:\n\nRegression on insurance premium prediction.\nClustering and association rule mining using the Iris dataset.\n\nThe primary research question focused on predicting values and identifying structure within datasets using ML algorithms.\n\nInsurance Dataset\n\n\nCode\nInsurance_df = pd.read_csv(\"data/train.csv\")\nInsurance_df.head(5)\n\n\n\n\n\n\n\n\n\nid\nAge\nGender\nAnnual Income\nMarital Status\nNumber of Dependents\nEducation Level\nOccupation\nHealth Score\nLocation\n...\nPrevious Claims\nVehicle Age\nCredit Score\nInsurance Duration\nPolicy Start Date\nCustomer Feedback\nSmoking Status\nExercise Frequency\nProperty Type\nPremium Amount\n\n\n\n\n0\n0\n19.0\nFemale\n10049.0\nMarried\n1.0\nBachelor's\nSelf-Employed\n22.598761\nUrban\n...\n2.0\n17.0\n372.0\n5.0\n2023-12-23 15:21:39.134960\nPoor\nNo\nWeekly\nHouse\n2869.0\n\n\n1\n1\n39.0\nFemale\n31678.0\nDivorced\n3.0\nMaster's\nNaN\n15.569731\nRural\n...\n1.0\n12.0\n694.0\n2.0\n2023-06-12 15:21:39.111551\nAverage\nYes\nMonthly\nHouse\n1483.0\n\n\n2\n2\n23.0\nMale\n25602.0\nDivorced\n3.0\nHigh School\nSelf-Employed\n47.177549\nSuburban\n...\n1.0\n14.0\nNaN\n3.0\n2023-09-30 15:21:39.221386\nGood\nYes\nWeekly\nHouse\n567.0\n\n\n3\n3\n21.0\nMale\n141855.0\nMarried\n2.0\nBachelor's\nNaN\n10.938144\nRural\n...\n1.0\n0.0\n367.0\n1.0\n2024-06-12 15:21:39.226954\nPoor\nYes\nDaily\nApartment\n765.0\n\n\n4\n4\n21.0\nMale\n39651.0\nSingle\n1.0\nBachelor's\nSelf-Employed\n20.376094\nRural\n...\n0.0\n8.0\n598.0\n4.0\n2021-12-01 15:21:39.252145\nPoor\nYes\nWeekly\nHouse\n2022.0\n\n\n\n\n5 rows × 21 columns\n\n\n\n\n\nIris Dataset\n\n\nCode\niris = load_iris()\niris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\niris_df.head(5)\n\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2"
  },
  {
    "objectID": "Assignment_Report.html#data-preprocessing",
    "href": "Assignment_Report.html#data-preprocessing",
    "title": "Comprehensive Machine Learning Reoprt",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nInsurance Dataset:\n\n1.2M rows, 20 columns.\nMissing values imputed using median/mode; duplicates removed. • Outliers detected in Annual Income and Premium Amount but retained.\n\nIris Dataset:\n\n150 rows, 4 features.\nNormalized and binned for association analysis. • Feature encoding applied where necessary."
  },
  {
    "objectID": "Assignment_Report.html#limitations-and-constrains",
    "href": "Assignment_Report.html#limitations-and-constrains",
    "title": "Comprehensive Machine Learning Reoprt",
    "section": "Limitations and Constrains",
    "text": "Limitations and Constrains\n\nRegression limited by low variance in normalized target variable.\nClustering relies on correct parameter tuning (eps, linkage).\nSmall sample in Iris dataset limits deep generalization.\nNo deep learning models due to computational constraints."
  },
  {
    "objectID": "Assignment_Report.html#findings",
    "href": "Assignment_Report.html#findings",
    "title": "Comprehensive Machine Learning Reoprt",
    "section": "Findings",
    "text": "Findings\n\nLinear Regression\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\nmodel = LinearRegression()\nmodel.fit(X_train, Y_train)\ny_pred = model.predict(X_test)\nprint(f\"Mean Squared Error: {mean_squared_error(Y_test,y_pred)}\")\nprint(f\"R2 Score: {r2_score(Y_test,y_pred)}\")\nprint(f\"Mean Absolute Error: {mean_absolute_error(Y_test,y_pred)}\")\n\n\nMean Squared Error: 0.030054850399406204\nR2 Score: 0.00296083662677904\nMean Absolute Error: 0.13399626402463904\n\n\n\n\nClassification Analysis\n\n\nCode\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\ndf_categorized = pd.read_csv(\"data/train_categorized.csv\")\ncat_features = [\"Gender\", \"Marital Status\", \"Education Level\", \"Occupation\",\n                \"Location\", \"Policy Type\", \"Smoking Status\",\n                \"Exercise Frequency\", \"Property Type\",\"Customer Feedback\",\n                \"Number of Dependents\", \"Previous Claims\", \"Insurance Duration\", \"Premium Amount Categorized\"]\nle = LabelEncoder()\ndf_encoded = df_categorized.copy()\nfor column in cat_features:\n    df_encoded[column] = le.fit_transform(df_categorized[column])\n\ndf_encoded=df_encoded.apply(pd.to_numeric)\nY = df_encoded['Premium Amount Categorized']\nX = df_encoded.drop(columns=['Premium Amount Categorized', 'Premium Amount'])\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n\n\nDecision Tree Classifier\n\n\nCode\ndecision_tree = DecisionTreeClassifier(random_state=42,class_weight='balanced')\ndecision_tree.fit(X_train, Y_train)\nY_pred_dt = decision_tree.predict(X_test)\ndecision_tree_score = decision_tree.score(X_test, Y_test)*100\nprint(f\"Decision Tree  Accuracy: {accuracy_score(Y_test, Y_pred_dt)*100:.2f}%\")\nprint(\"Key Metrics for Decision Tree Classifier:\")\nprint(classification_report(Y_test, Y_pred_dt))\n\n\nDecision Tree  Accuracy: 52.22%\nKey Metrics for Decision Tree Classifier:\n              precision    recall  f1-score   support\n\n           0       0.52      0.52      0.52    179512\n           1       0.52      0.52      0.52    180488\n\n    accuracy                           0.52    360000\n   macro avg       0.52      0.52      0.52    360000\nweighted avg       0.52      0.52      0.52    360000\n\n\n\n\n\nNaive Bayes Classifier\n\n\nCode\nfrom sklearn.naive_bayes import GaussianNB\nnaive_bayes = GaussianNB()\nnaive_bayes.fit(X_train, Y_train)\nY_pred_nb = naive_bayes.predict(X_test)\nnaive_bayes_score = naive_bayes.score(X_test, Y_test)*100\nprint(f\"Naive Bayes Accuracy: {accuracy_score(Y_test, Y_pred_nb)*100:.2f}%\")\nprint(\"Key Metrics for Naive Bayes Classifier:\")\nprint(classification_report(Y_test, Y_pred_nb))\n\n\nNaive Bayes Accuracy: 52.77%\nKey Metrics for Naive Bayes Classifier:\n              precision    recall  f1-score   support\n\n           0       0.53      0.45      0.49    179512\n           1       0.53      0.60      0.56    180488\n\n    accuracy                           0.53    360000\n   macro avg       0.53      0.53      0.53    360000\nweighted avg       0.53      0.53      0.53    360000\n\n\n\n\n\nk-Nearest Neighbours\n\n\nCode\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, Y_train)\nY_pred_knn = knn.predict(X_test)\nknn_score = knn.score(X_test, Y_test)*100\nprint(f\"k-NN Accuracy: {accuracy_score(Y_test, Y_pred_knn)*100:.2f}%\")\nprint(\"Key Metrics for k-NN Classifier:\")\nprint(classification_report(Y_test, Y_pred_knn))\n\n\nk-NN Accuracy: 50.41%\nKey Metrics for k-NN Classifier:\n              precision    recall  f1-score   support\n\n           0       0.50      0.50      0.50    179512\n           1       0.51      0.51      0.51    180488\n\n    accuracy                           0.50    360000\n   macro avg       0.50      0.50      0.50    360000\nweighted avg       0.50      0.50      0.50    360000\n\n\n\nSummary of Classification Models\n\n\nCode\nmodel_performances = {\n    'Decision Tree': decision_tree_score,\n    'Naive Bayes': naive_bayes_score,\n    'k-NN': knn_score\n}\nperformance_df = pd.DataFrame(list(model_performances.items()), columns=['Model', 'Accuracy (%)'])\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Model', y='Accuracy (%)', data=performance_df)\nplt.title('Model Performance Comparison')\nplt.ylim(0, 100)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCluster Analysis\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\nfrom sklearn.decomposition import PCA\n\n# Load iris dataset\niris_raw = load_iris()\niris_data = pd.DataFrame(iris_raw.data, columns=iris_raw.feature_names)\niris_data['species'] = iris_raw.target  # numeric target (0,1,2)\niris_data['species_name'] = [iris_raw.target_names[t] for t in iris_data['species']]\n\nlabel_encoders = {}\nle = LabelEncoder()\niris_data['species_encoded'] = le.fit_transform(iris_data['species_name'])\nlabel_encoders['species_name'] = le \niris_data[['species_name', 'species_encoded']].drop_duplicates()\n\nfeatures = iris_raw.feature_names\nX = iris_data[features].copy()\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled_df = pd.DataFrame(X_scaled, columns=features)\n\n# Set random state\nrandom_state = 42\n\n# k-Means\nkmeans = KMeans(n_clusters=3, random_state=random_state, n_init=10)\nlabels_kmeans = kmeans.fit_predict(X_scaled)\n\n# Agglomerative Clustering\nagg = AgglomerativeClustering(n_clusters=3, linkage='ward')\nlabels_agg = agg.fit_predict(X_scaled)\n\n# DBSCAN\ndbscan = DBSCAN(eps=0.6, min_samples=5)\nlabels_db = dbscan.fit_predict(X_scaled)\n\niris_data['cluster_kmeans'] = labels_kmeans\niris_data['cluster_agg'] = labels_agg\niris_data['cluster_dbscan'] = labels_db\n\npca = PCA(n_components=2, random_state=random_state)\npca_result = pca.fit_transform(X_scaled)\niris_data['pca1'] = pca_result[:, 0]\niris_data['pca2'] = pca_result[:, 1]\ndef plot_clusters(df, x='pca1', y='pca2', label_col='cluster', title='Clusters', palette='tab10', show_centers=None):\n    plt.figure(figsize=(6.8,5))\n    labels = df[label_col]\n    unique_labels = np.unique(labels)\n    palette_map = sns.color_palette(palette, n_colors=len(unique_labels))\n    color_dict = {lab: palette_map[i] for i, lab in enumerate(unique_labels)}\n    for lab in unique_labels:\n        mask = labels == lab\n        if lab == -1:\n            col = 'k'  # noise black\n            lab_name = 'noise'\n        else:\n            col = color_dict[lab]\n            lab_name = str(lab)\n        plt.scatter(df.loc[mask, x], df.loc[mask, y], label=lab_name, alpha=0.75, s=50)\n    if show_centers is not None:\n        plt.scatter(show_centers[:,0], show_centers[:,1], marker='X', s=150, c='red', edgecolor='k', label='centers')\n    plt.xlabel(x)\n    plt.ylabel(y)\n    plt.title(title)\n    plt.legend(title='cluster', bbox_to_anchor=(1.02,1), loc='upper left')\n    plt.tight_layout()\n    plt.show()\n\n\n\nK-Means\n\n\nCode\nk_centers_pca = pca.transform(kmeans.cluster_centers_)\nplot_clusters(iris_data, label_col='cluster_kmeans', title='K-Means clusters', show_centers=k_centers_pca)\n\n\n\n\n\n\n\n\n\n\n\nHeirarchical Agglomaration\n\n\nCode\nplot_clusters(iris_data, label_col='cluster_agg', title='Agglomerative clusters')\n\n\n\n\n\n\n\n\n\n\n\nDBSCAN\n\n\nCode\nplot_clusters(iris_data, label_col='cluster_dbscan', title='DBSCAN clusters')\n\n\n\n\n\n\n\n\n\n\n\nApriori Algorithm\n\n\nCode\n# Create a copy of iris data with species information\niris_binned = iris_data[iris_raw.feature_names].copy()\niris_binned['species'] = iris_data['species_name']\n\n# Bin numerical columns into categories\nnum_cols = iris_raw.feature_names\nfor col in num_cols:\n    iris_binned[col] = pd.qcut(iris_binned[col], q=3, labels=['Low','Medium','High'])\n\niris_binned['species'] = iris_binned['species'].astype(str)\n\n# Create transaction format\ndf_items = iris_binned.copy()\nfor col in df_items.columns:\n    df_items[col] = col + '=' + df_items[col].astype(str)\n\ntrans_df = pd.get_dummies(df_items)\n\nfrom mlxtend.frequent_patterns import apriori\nmin_support = 0.10\nfrequent_itemsets = apriori(trans_df, min_support=min_support, use_colnames=True)\n\nfrom mlxtend.frequent_patterns import association_rules\nmin_confidence = 0.6\nrules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=min_confidence)\nrules = rules.sort_values(by='confidence', ascending=False)\n\nprint(\"\\nTop Association Rule:\")\nprint(rules[['antecedents','consequents','support','confidence','lift']].head(5))\n\nspecies_rules = rules[rules['consequents'].astype(str).str.contains('species', case=False, na=False)]\nprint(\"\\nRules predicting Species (class labels):\")\nprint(species_rules[['antecedents', 'consequents', 'confidence', 'lift']].head(5))\n\n\n\nTop Association Rule:\n                                           antecedents  \\\n302  (petal width (cm)_petal width (cm)=Low, petal ...   \n303  (petal width (cm)_petal width (cm)=Low, specie...   \n272  (sepal width (cm)_sepal width (cm)=Medium, pet...   \n273  (sepal width (cm)_sepal width (cm)=Medium, pet...   \n278  (sepal width (cm)_sepal width (cm)=Medium, pet...   \n\n                                   consequents   support  confidence  lift  \n302                   (species_species=setosa)  0.333333         1.0   3.0  \n303  (petal length (cm)_petal length (cm)=Low)  0.333333         1.0   3.0  \n272  (petal length (cm)_petal length (cm)=Low)  0.100000         1.0   3.0  \n273    (petal width (cm)_petal width (cm)=Low)  0.100000         1.0   3.0  \n278                   (species_species=setosa)  0.100000         1.0   3.0  \n\nRules predicting Species (class labels):\n                                           antecedents  \\\n302  (petal width (cm)_petal width (cm)=Low, petal ...   \n278  (sepal width (cm)_sepal width (cm)=Medium, pet...   \n305            (petal width (cm)_petal width (cm)=Low)   \n306          (petal length (cm)_petal length (cm)=Low)   \n308  (petal width (cm)_petal width (cm)=Medium, pet...   \n\n                                           consequents  confidence  lift  \n302                           (species_species=setosa)         1.0   3.0  \n278                           (species_species=setosa)         1.0   3.0  \n305  (petal length (cm)_petal length (cm)=Low, spec...         1.0   3.0  \n306  (petal width (cm)_petal width (cm)=Low, specie...         1.0   3.0  \n308                       (species_species=versicolor)         1.0   3.0  \n\n\n\nInsurance regression had limited prediction accuracy (MSE: 0.03, R²: 0.0029).\nK-Means isolated Setosa clearly; other species overlapped.\nApriori found 100% confidence rules linking petal width/length to species."
  },
  {
    "objectID": "Assignment_Report.html#implications",
    "href": "Assignment_Report.html#implications",
    "title": "Comprehensive Machine Learning Reoprt",
    "section": "Implications",
    "text": "Implications\n\nProper preprocessing is critical for robust model building.\nSimple ML models are effective on clean, structured datasets.\nRule mining and clustering can reveal interpretable patterns aiding decision-making."
  },
  {
    "objectID": "Assignment_Report.html#insights-and-analysis",
    "href": "Assignment_Report.html#insights-and-analysis",
    "title": "Comprehensive Machine Learning Reoprt",
    "section": "Insights and Analysis",
    "text": "Insights and Analysis\n\nThis analysis demonstrates the value of diverse ML techniques:\nRegression helps quantify relationships.\nClustering identifies natural groupings.\nAssociation rules reveal hidden dependencies.\n\nTogether, these methods provide a holistic approach to understanding structured data."
  },
  {
    "objectID": "Assignment_Report.html#conclusion",
    "href": "Assignment_Report.html#conclusion",
    "title": "Comprehensive Machine Learning Reoprt",
    "section": "Conclusion",
    "text": "Conclusion\nThe overall research reinforces that effective preprocessing and appropriate model selection are crucial for success. Future work should explore feature engineering, ensemble models, and deep learning for enhanced prediction accuracy."
  },
  {
    "objectID": "Assignment_Report.html#references",
    "href": "Assignment_Report.html#references",
    "title": "Comprehensive Machine Learning Reoprt",
    "section": "References",
    "text": "References\n\nscikit-learn documentation: scikit\npandas, seaborn, matplotlib official docs\nOriginal Insurance Premium and Iris datasets"
  }
]